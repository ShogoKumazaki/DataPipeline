# データパイプライン構成提案 Q&A

> **日付**: 2026/02/09

---

## Q1. Cloud Run Jobs とデプロイ自動化（CI/CD）

### ご質問

> Cloud Run Jobs の活用や、Google Cloud Workflows による依存関係の制御について、最適な組み合わせを検討したいです。あわせて、運用面を考慮して Backlog のリポジトリから Cloud Run へ自動デプロイする CI/CD パイプラインの構築も構成に含めた方がいいと思うが、どうでしょうか？

### 回答

おっしゃる通り、**Cloud Run Jobs + Cloud Workflows** の組み合わせが最適だと考えます。CI/CD も含め、最終的にはすべて構成に盛り込みたいと思います。

#### パイプライン構成イメージ（最終形）

```
Cloud Scheduler（日次トリガー）
  → Cloud Workflows（実行順序・依存関係の制御）
    → Cloud Run Jobs（データ取得・CSV 作成・BQ ロード・スプレッドシート書き出し）
      → BigQuery Scheduled Query（データ整形）

Backlog（Git push）
  → Cloud Build Webhook トリガー
    → コンテナビルド → Cloud Run Jobs / Scheduled Query を自動デプロイ
```

#### CI/CD 構成

**Cloud Build の Webhook トリガー**を利用すれば、Backlog の Git リポジトリと連携可能です。

1. Backlog 側でプロジェクト設定から Webhook を登録（Git push イベント時に Cloud Build の Webhook URL へ通知）
2. Cloud Build が `cloudbuild.yaml` に従いコンテナビルド → Cloud Run Jobs へ自動デプロイ

#### 進め方について

なお、進め方としては **フェーズを分けて段階的に進める** のが良いかもしれません。

- **フェーズ 1**: パイプライン本体の実装を優先し、まず「データが毎日スプレッドシートに反映される」状態を実現する（Cloud Scheduler → Cloud Run Jobs の直接起動、GCS → BQ 転送やスケジュールクエリはコンソールから手動設定）
- **フェーズ 2**: 稼働後に Cloud Workflows の導入や CI/CD パイプラインの構築など、運用基盤を整備する（GCS → BQ 転送のスクリプト組み込みもこのタイミングで検討）

フェーズ 1 で実際に動かした知見をもとにフェーズ 2 を設計できるので、手戻りが少なくなるメリットもあります。

#### コストについて

Cloud Run Jobs / Cloud Workflows / Cloud Scheduler / Cloud Build いずれも無料枠が十分にあり、日次バッチ程度の利用であれば**月額ほぼ $0** で運用できる見込みです。「ほぼゼロコスト」という当初の提案メリットはそのまま維持されます。

---

## Q2. BigQuery Scheduled Query の管理について

### ご質問

> Step 3 の BigQuery スケジュール済みクエリについても、SQL ファイルを Backlog で管理し、CI/CD 経由で自動更新（デプロイ）される仕組みにしたいと考えています。このあたりの自動化も可能でしょうか？

### 回答

はい、**自動化可能です**。SQL ファイルを Backlog のリポジトリで管理し、CI/CD 経由でデプロイする仕組みを構築できます。

具体的には、**bq コマンド（BigQuery CLI）**を使ったアプローチを想定しています。

| 操作 | コマンド |
|------|---------|
| 作成 | `bq mk --transfer_config` でスケジュールクエリを登録 |
| 更新 | `bq update --transfer_config` で SQL の内容を更新 |
| 削除 | `bq rm --transfer_config` で不要になったクエリを削除 |

Cloud Build のパイプライン内で、リポジトリの SQL ファイルを読み取り → bq コマンドでスケジュールクエリを作成・更新する流れです。

Q1 の CI/CD パイプラインと同じ仕組み（Backlog Webhook → Cloud Build）で管理できるため、**運用が統一**されます。

> **進め方**: Q1 のフェーズ分けに沿い、フェーズ 1 ではコンソールから手動設定し、フェーズ 2 で CI/CD 管理に移が良いかと思います。。

https://docs.cloud.google.com/bigquery/docs/scheduling-queries?hl=ja#updating-a-scheduled-query

---

## Q3. データのリフレッシュ期間と「データの重複防止」

### ご質問

> Google 広告のコンバージョン期間の関係で、今は過去 90 日分をリフレッシュしています。BigQuery Data Transfer Service は 30 日までしか遡れず、タイミング制御も難しいため、別の手段が必要かと考えています。また、再取得時にデータが重複しないよう「常に最新データで上書き」できる仕組みにしたいです。

#### 対応方針

**Google Ads API から毎回過去 90 日分のデータを取得**し、CSV ファイルを作成します。

Raw テーブルへのロードは **DELETE + INSERT 方式**を採ります。過去データを蓄積しつつ、直近 90 日分だけを最新データにリフレッシュする方式です。

```
1. Raw テーブルから直近 90 日分のデータを DELETE
   DELETE FROM raw_table WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)

2. API から取得した最新 90 日分のデータを APPEND（追記）でロード
```

```
【イメージ】
DELETE 前: |--- 古いデータ(確定済み) ---|-- 前回取得の90日分 --|
DELETE 後: |--- 古いデータ(確定済み) ---|
INSERT 後: |--- 古いデータ(確定済み) ---|-- 最新取得の90日分 --|
```

これにより、以下を実現できます。

- **90 日分のコンバージョン遅延反映に対応**
- **90 日より古いデータは蓄積され続ける**（Raw テーブルに過去すべてのデータを保持）
- **削除と挿入の範囲が同じ 90 日のため、データの重複・欠損が発生しない**
- **複合キーの定義が不要でシンプルに運用できる**

---

## Q4. スプレッドシートへの出力（Step 4）

### ご質問

> Connected Sheets はライセンス制約があるため、短期的には Python スクリプトを Cloud Run に移植して API で書き出す「プッシュ型」が確実だと考えています。

### 回答

Connected Sheets（データコネクタ）のライセンス制約について詳しく把握できていないのですが、どういった制約で難しいでしょうか？

ただ、プッシュ型（Python スクリプトで Google Sheets API 経由の書き出し）でも問題ないと考えています。Workspace のエディションに依存しない点もメリットです。
