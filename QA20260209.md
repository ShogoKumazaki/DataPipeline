# データパイプライン構成提案 Q&A

> **日付**: 2026/02/09

---

## Q1. Cloud Run Jobs とデプロイ自動化（CI/CD）

### ご質問

> Cloud Run Jobs の活用や、Google Cloud Workflows による依存関係の制御について、最適な組み合わせを検討したいです。あわせて、運用面を考慮して Backlog のリポジトリから Cloud Run へ自動デプロイする CI/CD パイプラインの構築も構成に含めた方がいいと思うが、どうでしょうか？

### 回答

おっしゃる通り、**Cloud Run Jobs + Cloud Workflows** の組み合わせが最適だと考えます。

Cloud Run Jobs はバッチ処理向けに設計されており、Cloud Functions の最大 9 分のタイムアウトに対して**最大 168 時間（7日間）**まで対応できるため、データパイプラインとの相性が良いです。

#### パイプライン構成イメージ

```
Cloud Scheduler（日次トリガー）
  → Cloud Workflows（実行順序・依存関係の制御）
    → Cloud Run Jobs（データ取得・CSV 作成・BQ ロード）
      → BigQuery Scheduled Query（データ整形）
```

#### CI/CD 構成

CI/CD についても構成に含める方向で進めたいと思います。**Cloud Build の Webhook トリガー**を利用すれば、Backlog の Git リポジトリと連携可能です。

1. Backlog 側でプロジェクト設定から Webhook を登録（Git push イベント時に Cloud Build の Webhook URL へ通知）
2. Cloud Build が `cloudbuild.yaml` に従いコンテナビルド → Cloud Run Jobs へ自動デプロイ

---

## Q2. BigQuery Scheduled Query の管理について

### ご質問

> Step 3 の BigQuery スケジュール済みクエリについても、SQL ファイルを Backlog で管理し、CI/CD 経由で自動更新（デプロイ）される仕組みにしたいと考えています。このあたりの自動化も可能でしょうか？

### 回答

はい、**自動化可能です**。SQL ファイルを Backlog のリポジトリで管理し、CI/CD 経由でデプロイする仕組みを構築できます。

具体的には、**bq コマンド（BigQuery CLI）**を使ったアプローチを想定しています。

| 操作 | コマンド |
|------|---------|
| 作成 | `bq mk --transfer_config` でスケジュールクエリを登録 |
| 更新 | `bq update --transfer_config` で SQL の内容を更新 |
| 削除 | `bq rm --transfer_config` で不要になったクエリを削除 |

Cloud Build のパイプライン内で、リポジトリの SQL ファイルを読み取り → bq コマンドでスケジュールクエリを作成・更新する流れです。

Q1 の CI/CD パイプラインと同じ仕組み（Backlog Webhook → Cloud Build）で管理できるため、**運用が統一**されます。

https://docs.cloud.google.com/bigquery/docs/scheduling-queries?hl=ja#updating-a-scheduled-query

---

## Q3. データのリフレッシュ期間と「データの重複防止」

### ご質問

> Google 広告のコンバージョン期間の関係で、今は過去 90 日分をリフレッシュしています。BigQuery Data Transfer Service は 30 日までしか遡れず、タイミング制御も難しいため、別の手段が必要かと考えています。また、再取得時にデータが重複しないよう「常に最新データで上書き」できる仕組みにしたいです。

#### 対応方針

**Google Ads API から毎回過去 90 日分のデータを取得**し、CSV ファイルを作成します。BigQuery へのロード時にテーブルを丸ごと上書き（**WRITE_TRUNCATE** モード）する方式を採ります。

これにより、以下を実現できます。

- **90 日分のコンバージョン遅延反映に対応**
- **毎回全件上書きのため、データの重複が発生しない**
- **シンプルで運用しやすい**

---

## Q4. スプレッドシートへの出力（Step 4）

### ご質問

> Connected Sheets はライセンス制約があるため、短期的には Python スクリプトを Cloud Run に移植して API で書き出す「プッシュ型」が確実だと考えています。

### 回答

データコネクタのライセンス制約詳しく把握でいていないのですがどういった制約で難しいでしょうか？

また、プッシュ型でも問題ないと思います！

